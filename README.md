## Overview
This repository contains the implementation of various NLP tasks and models.

### Part 1: Classification and Regression

1. **Data Collection**:
   - Use web scraping libraries (Scrapy/BeautifulSoup) to collect Arabic text data on a specific topic from multiple websites.
   - Prepare a dataset with text and their corresponding relevance scores (0-10).

2. **Preprocessing**:
   - Establish an NLP preprocessing pipeline including tokenization, stemming, lemmatization, stop words removal, and discretization.

3. **Model Training**:
   - Train models using RNN, Bidirectional RNN, GRU, and LSTM architectures.
   - Tune hyper-parameters to achieve the best performance.

4. **Evaluation**:
   - Evaluate the models using standard metrics and other metrics like the BLEU score.

### Part 2: Transformer (Text Generation)

1. **Setup**:
   - Install PyTorch-Transformers and load the pre-trained GPT-2 model.

2. **Fine-Tuning**:
   - Fine-tune the GPT-2 model on a customized dataset (which can be generated by you).

3. **Text Generation**:
   - Generate new paragraphs based on given sentences.

### Part 3: BERT

1. **Model Setup**:
   - Use the pre-trained `bert-base-uncased` model.

2. **Data Preparation**:
   - Prepare the dataset and adapt the BERT embedding layer.

3. **Training**:
   - Fine-tune and train the model with optimal hyper-parameters.

4. **Evaluation**:
   - Evaluate the model using metrics such as accuracy, loss, F1 score, BLEU score, and BERT-specific metrics.

## Conclusion
Through this lab, I've learned about NLP, especially model training with architectures like RNN, LSTM, fine-tuning transformers "GPT-2", and using BERT for tasks like classification.
  
## Additional Resources

- [NLP Tutorial](https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7)
- [Amazon Dataset](https://nijianmo.github.io/amazon/index.html)

